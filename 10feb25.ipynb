{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10474341,"sourceType":"datasetVersion","datasetId":6485509}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom imblearn.over_sampling import SMOTE\nfrom scipy.stats import randint, uniform\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/finaldata/Final.csv')\n\n# Encoding categorical labels\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\n\n# Feature-target split\nX = df.drop(columns=['label'])\ny = df['label']\n\n# Handle class imbalance\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Splitting dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Hyperparameter tuning for Random Forest\nrf_params = {\n    'n_estimators': [400, 500, 600],  \n    'max_depth': [30, 40, 50],  \n    'min_samples_split': [2, 5, 10], \n    'min_samples_leaf': [1, 2, 5]\n}\n\nrf = RandomForestClassifier(random_state=42)\nrf_random = RandomizedSearchCV(rf, rf_params, n_iter=15, cv=5, n_jobs=-1, scoring='accuracy', random_state=42)\nrf_random.fit(X_train_scaled, y_train)\nbest_rf = rf_random.best_estimator_\n\n# Hyperparameter tuning for LightGBM\nlgbm_params = {\n    'n_estimators': [400, 500, 600],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [10, 15, 20],\n    'num_leaves': [31, 40, 50]\n}\n\nlgbm = LGBMClassifier()\nlgbm_random = RandomizedSearchCV(lgbm, lgbm_params, n_iter=15, cv=5, n_jobs=-1, scoring='accuracy', random_state=42)\nlgbm_random.fit(X_train_scaled, y_train)\nbest_lgbm = lgbm_random.best_estimator_\n\n# Hyperparameter tuning for XGBoost\nxgb_params = {\n    'n_estimators': [400, 500, 600],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [10, 15, 20],\n    'subsample': [0.7, 0.8, 0.9]\n}\n\nxgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\nxgb_random = RandomizedSearchCV(xgb, xgb_params, n_iter=15, cv=5, n_jobs=-1, scoring='accuracy', random_state=42)\nxgb_random.fit(X_train_scaled, y_train)\nbest_xgb = xgb_random.best_estimator_\n\n# Building MLP Neural Network Model\nmlp_model = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(len(np.unique(y_train)), activation='softmax')\n])\n\nmlp_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmlp_model.fit(X_train_scaled, y_train, epochs=30, batch_size=32, validation_data=(X_test_scaled, y_test), verbose=1)\n\n# Predict with MLP model\nmlp_pred = np.argmax(mlp_model.predict(X_test_scaled), axis=1)\n\n# Stacking Classifier with stronger meta-classifier\nstacking_clf = StackingClassifier(\n    estimators=[('rf', best_rf), ('lgbm', best_lgbm), ('xgb', best_xgb)],\n    final_estimator=GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, random_state=42)\n)\n\nstacking_clf.fit(X_train_scaled, y_train)\nstacking_pred = stacking_clf.predict(X_test_scaled)\n\n# Print accuracy\nprint(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, stacking_pred))\nprint(\"MLP Model Accuracy:\", accuracy_score(y_test, mlp_pred))\n\n# Print classification report\nprint(\"Stacking Classifier Report:\\n\", classification_report(y_test, stacking_pred))\nprint(\"MLP Model Report:\\n\", classification_report(y_test, mlp_pred))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-14T07:37:22.529177Z","iopub.execute_input":"2025-02-14T07:37:22.529552Z"}},"outputs":[],"execution_count":null}]}